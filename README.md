# SupConLoss Fake Image Detection

A PyTorch implementation of supervised contrastive learning for the AI-generated detection and source attribution problems. This repository contains the code and experiments for applying SupConLoss to the challenging task of distinguishing real images from synthetic ones generated by various AI models and attributing them to their generator.

## ğŸ¯ Purpose

The proliferation of AI-generated images poses significant challenges for content authenticity and trustworthiness. This project applies **Supervised Contrastive Learning (SupConLoss)** to build robust fake image detectors that can:

- Distinguish between real and AI-generated images
- Generalize across different generative models (GANs, Diffusion models, etc.)
- Learn discriminative representations through contrastive learning
- Provide better feature separability compared to traditional cross-entropy methods

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Training](#training)
- [Evaluation](#evaluation)
- [Results](#results)
- [Citation](#citation)
- [License](#license)

## ğŸš€ Installation

### Prerequisites

- Python 3.11+
- CUDA 11.8+ (for GPU support)
- Docker (optional, but recommended)

### Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/JaimeAlvarez18/SupConLoss_fake_image_detection.git
   cd SupConLoss_fake_image_detection
   ```

2. **Create a virtual environment** (if not using Docker)
   ```bash
   conda create -n supcon python=3.11 -y
   conda activate supcon
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

   Key dependencies include:
   - PyTorch 2.6.0+cu118
   - torchvision 0.21.0+cu118
   - timm (PyTorch Image Models)
   - transformers
   - scikit-learn
   - numpy, pandas, matplotlib

### Docker Setup (Recommended)

If you're using a PyTorch Docker container:
```bash
docker pull pytorch/pytorch:2.6.0-cuda11.8-cudnn9-runtime
docker run -it --gpus all -v $(pwd):/workspace pytorch/pytorch:2.6.0-cuda11.8-cudnn9-runtime
cd /workspace
pip install -r requirements.txt
```

## ğŸ“Š Datasets

### Important Note
âš ï¸ **The datasets included in this repository are subsampled versions for demonstration purposes only.** They are not the complete datasets used in the paper.



## ğŸ“ Project Structure

```
SupConLoss_fake_image_detection/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ supervised_contrastive_learning.py       # Main training script         
â”‚   â””â”€â”€ config.yaml                              # Training hyperparameters
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_binary.py                           # Binary metrics
â”‚   â”œâ”€â”€ test_source_attribution.py               # Source attribution metrics
â”‚   â”œâ”€â”€ test_open_set_classification.py          # Open set metrics
â”‚   â””â”€â”€ config.yaml                              # Testing hyperparameters
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ GenImage_sampled/                        # GenImage dataset (subsampled)
â”‚   â””â”€â”€ ForenSynths_sampled/                     # ForenSynths dataset (subsampled)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_acquisition.py                      # Data loading utilities
â”‚   â””â”€â”€ loss.py                                  # Supevised Contrastive Loss                           
â”œâ”€â”€ requirements.txt                             # Python dependencies
â””â”€â”€ README.md                                  
```

## ğŸ® Usage

### Quick Start

1. **Prepare your dataset** (or use the provided subsampled data for testing)
2. **Configure training parameters** in your training script
3. **Run training**
4. **Evaluate the model**

### Training

To train the model with supervised contrastive learning:

```bash
python -m training.supervised_contrastive_learning
```

### Evaluation

Evaluate the trained model on the test sets:

```bash
python -m tests.test_binary 
python -m tests.test_open_set_classification 
python -m tests.test_source_attribution 

```


## ğŸ“Š Results

### Comparison with State-of-the-Art Models

Performance comparison on AI-generated image detection across different generative models:

| Model | ADM | BigGan | Glide | Midjourney | VQDM | SD 1.4 | SD 1.5 | Wukong | Average |
|-------|-----|--------|-------|------------|------|--------|--------|--------|---------|
|  [1] | 68.1 | 95.3 | 64.0 | 57.4 | 85.2 | 61.2 | 63.0 | 71.3 | 70.7 |
|  [2] | 76.4 | 67.1 | 72.4 | 58.4 | 54.4 | 49.6 | 49.8 | 55.4 | 60.4 |
|  [3] | 87.1 | 94.2 | **96.3** | 65.1 | **95.6** | **98.0** | **97.1** | 55.4 | 86.1 |
|  [4] | 61.4 | 82.1 | 70.8 | 67.4 | 67.8 | 63.0 | 64.2 | 70.8 | 68.4 |
| **Ours** | **87.5** | **96.5** | 87.7 | **91.6** | 86.8 | 94.4 | 94.1 | **91.6** | **91.3** |

*Table: Binary classification accuracy (%) on various AI-generated image datasets. Bold values indicate best performance per column.*

**Key Findings:**
- Our model achieves the highest average accuracy of **91.3%** across all datasets
- Strong generalization across diverse generative models (ADM, BigGan, Glide, Midjourney, VQDM, Stable Diffusion, Wukong)
- Particularly effective on challenging datasets like Midjourney (91.6%) where other methods struggle



### Source Attribution Performance

Performance comparison on the source attribution benchmark dataset for multi-class generative model identification:

| Model | Closed Acc | Open AUC | Open OSCR |
|-------|------------|----------|-----------|
| [5] | 93.83 Â± 7.72 | 61.27 Â± 6.70 | 75.08 Â± 11.02 |
| [6] | 88.98 Â± 4.36 | 61.93 Â± 4.26 | 57.92 Â± 1.73 |
| [7] | 70.00 Â± 25.95 | 67.00 Â± 6.08 | 53.35 Â± 19.82 |
| [8] | **97.82 Â± 2.51** | 81.39 Â± 3.28 | 80.78 Â± 4.02 |
| **Average (Ours)** | 97.33 Â± 4.33 | **96.09 Â± 0.53** | **85.05 Â± 3.30** |

#### Individual Split Results (Ours)

| Split | Closed Acc | Open AUC | Open OSCR |
|-------|------------|----------|-----------|
| Split-1 | 89.82 | 95.16 | 79.36 |
| Split-2 | 99.67 | 96.48 | 86.46 |
| Split-3 | 99.91 | 96.30 | 87.00 |
| Split-4 | 99.93 | 96.40 | 87.43 |

*Table: Source attribution results showing Closed-set Accuracy, Open-set AUC, and Open-set Classification Rate (OSCR). Bold values indicate best performance per metric.*

**Key Findings:**
- **Open AUC**: Our method achieves **96.09%**, significantly outperforming the previous best (81.39%)
- **Open OSCR**: Achieves **85.05%**, demonstrating strong open-set recognition capability
- **Consistency**: Low standard deviation (Â±0.53 on Open AUC) shows robust performance across splits
- Competitive closed-set accuracy while excelling at open-set scenarios

### Metrics Explanation
- **Closed Acc**: Classification accuracy on known generative models
- **Open AUC**: Area under ROC curve for detecting unknown generative models
- **Open OSCR**: Open-Set Classification Rate, combining closed-set accuracy with open-set detection

### References

- [1] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models 
- [2] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection 
- [3] Juncong Xu, Yang Yang, Han Fang, Honggu Liu, and Weiming Zhang. Famsec: A few-shot-sample-based general
ai-generated image detection method.
- [4] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei. Learning on gradients: Generalized
artifacts representation for gan-generated images detection
- [5] Tianyun Yang, Ziyao Huang, Juan Cao, Lei Li, and Xirong Li. Deepfake network architecture attribution. 
- [6] Tu Bui, Ning Yu, and John Collomosse. Repmix: Representation mixing for robust attribution of synthesized
images. 
- [7] Tianyun Yang, Danding Wang, Fan Tang, Xinying Zhao, Juan Cao, and Sheng Tang. Progressive open space
expansion for open-set model attribution. 
- [8] Dario Cioni, Christos Tzelepis, Lorenzo Seidenari, and Ioannis Patras. Are clip features all you need for universal
synthetic image origin attribution? 


## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub or contact:
- GitHub: [@JaimeAlvarez18](https://github.com/JaimeAlvarez18)


**Note**: The datasets included in this repository are subsampled for demonstration purposes. Please download the full datasets for reproducing the paper results.